{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601411392187",
   "display_name": "Python 3.7.7 64-bit ('tf2.3-gpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from pyarabic.araby import strip_tashkeel,strip_tatweel,normalize_ligature\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['DIAL_EGY', 'DIAL_GLF', 'DIAL_LEV', 'MSA']\n",
    "label2id={label:idx for idx,label in enumerate(labels)}\n",
    "id2label={idx:label for label,idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sequence):\n",
    "    outputs=[]\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for tweet in sequence:\n",
    "        tweet = str(tweet)\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        # remove hashtags\n",
    "        # only removing the hash # sign from the word\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'@[\\w]+','',tweet)\n",
    "        #replace punctuations with space\n",
    "        tweet = re.sub(r\"[,.;@#?!&$_]+\\ *\", \" \", tweet)\n",
    "        #find arabic letters only\n",
    "        tweet = ' '.join(re.findall(r'[\\u0600-\\u06FF]+',tweet))\n",
    "        #remove tashkeel\n",
    "        tweet = strip_tashkeel(tweet)\n",
    "        #remove tatweel\n",
    "        tweet = strip_tatweel(tweet)\n",
    "        #apply normalization\n",
    "        #tweet = normalize_ligature(tweet)\n",
    "        #tokenize tweets\n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        #tweet_tokens = tweet.split(' ')\n",
    "        tweet_clean=[]\n",
    "        for word in tweet_tokens: # Go through every word in your tokens list\n",
    "            #if word not in string.punctuation:  # remove punctuation\n",
    "            #    tweet_clean.append(word)\n",
    "            word_reg = re.compile(r'\\w')\n",
    "            if word_reg.search(word):\n",
    "                tweet_clean.append(word)\n",
    "        outputs.append((' '.join(tweet_clean)))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing TFBertModel: ['mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the model checkpoint at asafaya/bert-base-arabic.\nIf your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "model = TFAutoModel.from_pretrained(\"asafaya/bert-base-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweets, tokenizer, max_seq_len = 128):\n",
    "    tokenized_tweets = []\n",
    "    oov_tokens=set()\n",
    "    for tweet in tqdm(tweets):\n",
    "        tokenized_tweet = tokenizer.encode(\n",
    "                            tweet,                  # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_seq_len,  # Truncate all sentences.\n",
    "                            truncation=True\n",
    "                    )\n",
    "        #oov_tokens.update([w for w in tweet.split(' ') if w not in tokenizer.get_vocab()])\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "    #print('num of oov',len(oov_tokens))\n",
    "    return tokenized_tweets\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_tweets):\n",
    "    attention_masks = []\n",
    "    for tweet in tokenized_and_padded_tweets:\n",
    "        att_mask = [int(token_id > 0) for token_id in tweet]\n",
    "        attention_masks.append(att_mask)\n",
    "    return np.asarray(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(tf.keras.Model):\n",
    "    def __init__(self,bert,units,rate=0.0):\n",
    "        super(BertModel,self).__init__()\n",
    "        self.bert=bert\n",
    "        self.dropout=tf.keras.layers.Dropout(rate)\n",
    "        self.dense=tf.keras.layers.Dense(4,activation='softmax')\n",
    "\n",
    "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "        x=self.bert(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,position_ids=position_ids,head_mask=head_mask)#(batch,seqlength,emb_dim)\n",
    "        outputs=self.dropout(x[1])\n",
    "        outputs=self.dense(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_model=BertModel(model,len(labels),0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1/1 [00:00<?, ?it/s]DIAL_EGY\n\n"
    }
   ],
   "source": [
    "def inference(model,tokenizer,input_text):\n",
    "    input_text = preprocess_text([input_text])\n",
    "    input_text_tokenized = tokenize_tweets(input_text,tokenizer)\n",
    "    input_text_padded = tf.keras.preprocessing.sequence.pad_sequences(input_text_tokenized,128)\n",
    "    input_text_att_mask = create_attention_masks(input_text_padded)\n",
    "    outputs = model(input_ids=input_text_padded, attention_mask=input_text_att_mask, training=False)[0]\n",
    "    return (id2label[outputs.numpy().argmax(axis=-1)])\n",
    "    \n",
    "DI_model.load_weights('models/BertBase asafaya/')\n",
    "prediction = inference(DI_model,tokenizer,\"انا بحب  كده\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}