{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from pyarabic.araby import strip_tashkeel,strip_tatweel,normalize_ligature\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv('MultiTrain.Shuffled.csv')\n",
    "test_dataset=pd.read_csv('MultiTest.csv')\n",
    "dev_dataset=pd.read_csv('MultiDev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Unnamed: 0     label                                               text\n0           0       MSA  بالإضافة لقيام معلمو الجيزة للذهاب إلي جريدة ا...\n1           1       MSA  بعدين والله حرام تجي تلقى الي واقف عند الاشاره...\n2           2  DIAL_LEV                   لمسه اليد مرتين واضحة جدا والحكم\n3           3  DIAL_LEV                   بخصوص الهاتريك عمرها ما راح تصير\n4           4  DIAL_GLF      الله يجبر كسرهم ويرجع و لدهم اليوم قبل بكرى ،",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>MSA</td>\n      <td>بالإضافة لقيام معلمو الجيزة للذهاب إلي جريدة ا...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>MSA</td>\n      <td>بعدين والله حرام تجي تلقى الي واقف عند الاشاره...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>DIAL_LEV</td>\n      <td>لمسه اليد مرتين واضحة جدا والحكم</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>DIAL_LEV</td>\n      <td>بخصوص الهاتريك عمرها ما راح تصير</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>DIAL_GLF</td>\n      <td>الله يجبر كسرهم ويرجع و لدهم اليوم قبل بكرى ،</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_dataset.iloc[:,1:].values\n",
    "test_data=test_dataset.iloc[:,1:].values\n",
    "dev_data=dev_dataset.iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=list(np.unique(train_data[:,0]))\n",
    "label2id={label:idx for idx,label in enumerate(labels)}\n",
    "id2label={idx:label for label,idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sequence):\n",
    "    outputs=[]\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for tweet in sequence:\n",
    "        tweet = str(tweet)\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "        # remove hashtags\n",
    "        # only removing the hash # sign from the word\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'@[\\w]+','',tweet)\n",
    "        #replace punctuations with space\n",
    "        tweet = re.sub(r\"[,.;@#?!&$_]+\\ *\", \" \", tweet)\n",
    "        #find arabic letters only\n",
    "        tweet = ' '.join(re.findall(r'[\\u0600-\\u06FF]+',tweet))\n",
    "        #remove tashkeel\n",
    "        tweet = strip_tashkeel(tweet)\n",
    "        #remove tatweel\n",
    "        tweet = strip_tatweel(tweet)\n",
    "        #apply normalization\n",
    "        #tweet = normalize_ligature(tweet)\n",
    "        #tokenize tweets\n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        #tweet_tokens = tweet.split(' ')\n",
    "        tweet_clean=[]\n",
    "        for word in tweet_tokens: # Go through every word in your tokens list\n",
    "            #if word not in string.punctuation:  # remove punctuation\n",
    "            #    tweet_clean.append(word)\n",
    "            word_reg = re.compile(r'\\w')\n",
    "            if word_reg.search(word):\n",
    "                tweet_clean.append(word)\n",
    "        outputs.append((' '.join(tweet_clean)))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:,1]=preprocess_text(train_data[:,1])\n",
    "test_data[:,1]=preprocess_text(test_data[:,1])\n",
    "dev_data[:,1]=preprocess_text(dev_data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labels=[label2id[label] for label in train_data[:,0]]\n",
    "test_data_labels=[label2id[label] for label in test_data[:,0]]\n",
    "dev_data_labels=[label2id[label] for label in dev_data[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing TFBertModel: ['mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the model checkpoint at asafaya/bert-base-arabic.\nIf your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "model = TFAutoModel.from_pretrained(\"asafaya/bert-base-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweets, tokenizer, max_seq_len = 128):\n",
    "    tokenized_tweets = []\n",
    "    oov_tokens=set()\n",
    "    for tweet in tqdm(tweets):\n",
    "        tokenized_tweet = tokenizer.encode(\n",
    "                            tweet,                  # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_seq_len,  # Truncate all sentences.\n",
    "                            truncation=True\n",
    "                    )\n",
    "        #oov_tokens.update([w for w in tweet.split(' ') if w not in tokenizer.get_vocab()])\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "    #print('num of oov',len(oov_tokens))\n",
    "    return tokenized_tweets\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_tweets):\n",
    "    attention_masks = []\n",
    "    for tweet in tokenized_and_padded_tweets:\n",
    "        att_mask = [int(token_id > 0) for token_id in tweet]\n",
    "        attention_masks.append(att_mask)\n",
    "    return np.asarray(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 86541/86541 [00:20<00:00, 4138.99it/s]\n100%|██████████| 10812/10812 [00:02<00:00, 3815.59it/s]\n100%|██████████| 10820/10820 [00:02<00:00, 3962.51it/s]\n"
    }
   ],
   "source": [
    "#tokenize tweets\n",
    "train_tokenized_tweets=tokenize_tweets(train_data[:,1],tokenizer)\n",
    "test_tokenized_tweets=tokenize_tweets(test_data[:,1],tokenizer)\n",
    "dev_tokenized_tweets=tokenize_tweets(dev_data[:,1],tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad tweets\n",
    "train_tokenized_padded_tweets=tf.keras.preprocessing.sequence.pad_sequences(train_tokenized_tweets,128)\n",
    "test_tokenized_padded_tweets=tf.keras.preprocessing.sequence.pad_sequences(test_tokenized_tweets,128)\n",
    "dev_tokenized_padded_tweets=tf.keras.preprocessing.sequence.pad_sequences(dev_tokenized_tweets,128)\n",
    "#attention_mask\n",
    "train_att_mask=create_attention_masks(train_tokenized_padded_tweets)\n",
    "test_att_mask=create_attention_masks(test_tokenized_padded_tweets)\n",
    "dev_att_mask=create_attention_masks(dev_tokenized_padded_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_tokenized_padded_tweets,train_att_mask,train_data_labels)).batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
    "validate_data = tf.data.Dataset.from_tensor_slices((dev_tokenized_padded_tweets,dev_att_mask,dev_data_labels)).batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_tokenized_padded_tweets,test_att_mask,test_data_labels)).batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(tf.keras.Model):\n",
    "    def __init__(self,bert,units,rate=0.0):\n",
    "        super(BertModel,self).__init__()\n",
    "        self.bert=bert\n",
    "        self.dropout=tf.keras.layers.Dropout(rate)\n",
    "        self.dense=tf.keras.layers.Dense(4,activation='softmax')\n",
    "\n",
    "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "        x=self.bert(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,position_ids=position_ids,head_mask=head_mask)#(batch,seqlength,emb_dim)\n",
    "        outputs=self.dropout(x[1])\n",
    "        outputs=self.dense(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_model=BertModel(model,len(labels),0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function and Accuracy measures\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(tweet,att_mask,label):\n",
    "    with tf.GradientTape() as g:\n",
    "        outputs=DI_model(tweet,att_mask,training=True)  \n",
    "        loss=loss_object(label,outputs)\n",
    "    train_loss.update_state(loss)\n",
    "    train_accuracy.update_state(label,outputs)\n",
    "    gradients=g.gradient(loss,DI_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,DI_model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def val_step(tweet,att_mask,label):\n",
    "    outputs=DI_model(tweet,att_mask,training=False)\n",
    "    loss=loss_object(label,outputs)\n",
    "    val_loss.update_state(loss)\n",
    "    val_accuracy.update_state(label,outputs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def test_step(saved_model,tweet,att_mask,label):\n",
    "    outputs=saved_model(input_ids=tweet,attention_mask=att_mask,training=False)\n",
    "    loss=loss_object(label,outputs)\n",
    "    test_loss.update_state(loss)\n",
    "    test_accuracy.update_state(label,outputs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_step(saved_model,tweet,att_mask):\n",
    "    outputs=saved_model(input_ids=tweet,attention_mask=att_mask,training=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS=100\n",
    "min_val_loss=99999\n",
    "patience=1\n",
    "for epoch in range(EPOCHS):\n",
    "    if patience > 0 :\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "\n",
    "        for batch_idx,(tweet,att_masks,outputs) in enumerate(train_data):\n",
    "            loss=train_step(tweet,att_masks,outputs)\n",
    "            if (batch_idx+1)%1000==0:\n",
    "                print('Epoch {} Batch {} Loss {}'.format(epoch+1,batch_idx+1,loss.numpy()))\n",
    "\n",
    "        for tweet,att_masks,outputs in validate_data:\n",
    "            val_step(tweet,att_masks,outputs)\n",
    "\n",
    "        template='Epoch {}, Train Loss {:.5}, Train Accuracy {:.5}, Val Loss {:.5}, Val Accuracy {:.5}'\n",
    "        print(template.format(epoch+1,train_loss.result(),train_accuracy.result(),val_loss.result(),val_accuracy.result()))\n",
    "\n",
    "        if val_loss.result() < min_val_loss:\n",
    "            DI_model.save_weights('models/BertBase asafaya/')\n",
    "            patience=1\n",
    "            print('new model has been saved')\n",
    "            min_val_loss=val_loss.result()\n",
    "        else:\n",
    "            patience-=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results\n",
    "\n",
    "DR = 0.0\n",
    "**test_acc = 0.8424**\n",
    "**test_precision = 0.8198**\n",
    "**test_recall = 0.7611**\n",
    "**test_f1 = 0.7872**\n",
    "\n",
    "DR = 0.3\n",
    "**test_acc = 0.8391**\n",
    "**test_precision = 0.8166**\n",
    "**test_recall = 0.7679**\n",
    "**test_f1 = 0.7882**\n",
    "\n",
    "DR = 0.5\n",
    "**test_acc = 0.8429**\n",
    "**test_precision = 0.8204**\n",
    "**test_recall = 0.7638**\n",
    "**test_f1 = 0.7889**\n",
    "\n",
    "DR = 0.8\n",
    "**test_acc = 0.8396**\n",
    "**test_precision = 0.8173**\n",
    "**test_recall = 0.7584**\n",
    "**test_f1 = 0.7844**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "from sklearn.metrics import classification_report\n",
    "DI_model.load_weights('models/BertBase asafaya/')\n",
    "predictions=None\n",
    "for index,(inputs,att_mask,labels) in enumerate(test_data):\n",
    "    pred = pred_step(DI_model,inputs,att_mask).numpy()\n",
    "    if index==0:\n",
    "        predictions=pred\n",
    "    else:\n",
    "        predictions = np.concatenate((predictions,pred),axis=0)\n",
    "print(classification_report(test_data_labels, predictions.argmax(axis=-1),target_names=list(label2id.keys()),digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python_defaultSpec_1601411246205",
   "display_name": "Python 3.7.7 64-bit ('tf2.3-gpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}